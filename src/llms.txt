# Drummond Geometry Analysis System (DGAS)
# LLM-Optimized Codebase Reference

## Project Overview
Local-first market data ingestion and Drummond Geometry analysis toolkit implementing multi-timeframe coordination for trading signals. Production-ready system with prediction engine, real-time dashboard, CLI tools, and performance optimizations.

**Stack**: Python 3.11+, PostgreSQL, EODHD API, pandas/numpy, rich (CLI), streamlit (dashboard), apscheduler (scheduling)

---

## Core Package

**dgas/__init__.py**
- `get_version() -> str`: Return installed package version via importlib.metadata

**dgas/__main__.py**
- `build_parser() -> ArgumentParser`: CLI structure with subcommands (analyze, backtest, configure, data, predict, report, scheduler, status, monitor, data-report)
- `main(argv: list[str] | None) -> int`: Route to subcommands or --version
- `_parse_key_value_pairs(items: list[str]) -> dict[str, str]`: Parse key=value strategy params

**dgas/settings.py**
- `Settings(BaseSettings)`: Pydantic config loader
  - Fields: `eodhd_api_token: str | None`, `database_url: str`, `data_dir: Path`, `eodhd_requests_per_minute: int`
- `get_settings() -> Settings`: Cached singleton via @lru_cache

---

## Configuration System

**dgas/config/schema.py**
- `DatabaseConfig(BaseModel)`: url, pool_size, echo
- `SchedulerConfig(BaseModel)`: symbols, cron_expression, timezone, market_hours_only
- `PredictionConfig(BaseModel)`: min_confidence, min_signal_strength, stop_loss_atr_multiplier, target_atr_multiplier
- `DiscordConfig(BaseModel)`: enabled, webhook_url
- `ConsoleConfig(BaseModel)`: enabled
- `NotificationConfig(BaseModel)`: discord, console
- `MonitoringConfig(BaseModel)`: sla_p95_latency_ms, sla_error_rate_pct, sla_uptime_pct
- `DashboardConfig(BaseModel)`: port, theme, auto_refresh_seconds
- `DGASConfig(BaseModel)`: Root config combining all sub-configs

**dgas/config/loader.py**
- `ConfigLoader(config_path: Path | None)`: Load and merge config from multiple sources
  - `find_config_file() -> Path | None`: Search default paths (/etc/dgas, ~/.config/dgas, ./)
  - `load_file(path: Path) -> dict[str, Any]`: Load YAML/JSON with env var expansion
  - `load() -> DGASConfig`: Load and validate config
  - `reload() -> DGASConfig`: Force reload
  - `save(config: DGASConfig, path: Path, format: str) -> None`: Save config to file
  - `generate_sample_config(path: Path, template: str) -> None`: Generate sample configs
- `load_config(config_path: Path | None) -> DGASConfig`: Convenience function

**dgas/config/adapter.py**
- `UnifiedSettings(config_file: Path | None, config_overrides: dict[str, Any] | None)`: Bridge DGASConfig and legacy Settings
  - Properties: database_url, database_pool_size, scheduler_symbols, prediction_min_confidence, notifications_*, monitoring_*, dashboard_*, eodhd_api_token, data_dir, eodhd_requests_per_minute
  - `to_dict() -> dict[str, Any]`: Convert to dictionary
- `load_settings(config_file: Path | None, **overrides: Any) -> UnifiedSettings`: Convenience function

**dgas/config/validators.py**
- `expand_env_vars_in_dict(data: dict) -> dict`: Expand ${VAR} in config values
- `validate_config_file(path: Path) -> None`: Validate file exists and is readable

---

## Core Utilities

**dgas/core/config_manager.py**
- `ServerConfig`: Configuration for server resources (num_cpus, db_pool_size, batch_io_size, memory_limit_mb)
- `get_optimal_config() -> ServerConfig`: Auto-detect optimal configuration based on system resources
- `get_config() -> ServerConfig`: Get or create the global configuration

**dgas/core/connection_pool.py**
- `DatabaseConnectionPool(pool_size: int)`: Simple connection pool using queue
  - `initialize() -> None`: Initialize the connection pool with connections
  - `get_connection(timeout: float) -> Generator[Connection]`: Get connection from pool
  - `close_all() -> None`: Close all connections
- `get_connection_pool(pool_size: int | None) -> DatabaseConnectionPool`: Global singleton
- `get_pooled_connection(timeout: float) -> Generator[Connection]`: Context manager for pooled connection

**dgas/core/parallel_processor.py**
- `BatchResult`: Result of processing a batch (batch_num, success, elapsed_seconds, error_msg, symbols)
- `ParallelBatchProcessor(max_workers: int)`: Process multiple backtest batches in parallel
  - `process_batches(batches: List[Tuple[int, List[str]]], batch_processor_func) -> List[BatchResult]`: Process batches in parallel
- `run_backtest_batch_subprocess(batch_num: int, symbols: List[str], total_batches: int) -> BatchResult`: Worker function for subprocess execution

**dgas/core/io_optimizer.py**
- `BatchIOWriter(batch_size: int)`: Batch database writes for improved performance
  - `add_result(result: Dict[str, Any]) -> None`: Add result to batch buffer
  - `flush_all() -> None`: Flush all remaining results
  - `get_stats() -> Dict[str, int]`: Get I/O statistics
- `MemoryMonitor(memory_limit_mb: int)`: Monitor and manage memory usage
  - `start_monitoring() -> None`: Start memory monitoring
  - `check_memory(force_gc: bool) -> None`: Check memory and trigger GC if needed
  - `get_stats() -> Dict[str, float]`: Get memory statistics
- `ParallelFileWriter(max_workers: int)`: Write results to files in parallel
  - `write_results(results: List[Dict[str, Any]], output_dir: str) -> None`: Write results in parallel
- `optimize_database_queries() -> None`: Apply database query optimizations
- `create_database_indexes() -> None`: Create indexes for better query performance
- `get_optimal_batch_size(symbol_count: int, num_workers: int) -> int`: Calculate optimal batch size

**dgas/utils/market_hours_filter.py**
- `filter_to_regular_hours(bars: Sequence[IntervalData], exchange_code: str) -> List[IntervalData]`: Filter bars to regular trading hours
- `is_during_regular_hours(timestamp: datetime, exchange_code: str) -> bool`: Check if timestamp is during regular hours
- `get_regular_hours_stats(bars: Sequence[IntervalData], exchange_code: str) -> dict`: Get statistics about regular hours coverage

---

## Database Layer

**dgas/db/__init__.py**
- `get_connection() -> Iterator[psycopg.Connection]`: Context manager for PostgreSQL transactions

**dgas/db/migrations.py**
- `list_migration_files() -> Iterable[Path]`: Sorted *.sql from migrations/
- `apply_all() -> None`: Execute pending migrations sequentially
- `main() -> None`: CLI entry for migration runner

**dgas/db/persistence.py**
- `DrummondPersistence(settings: Settings | None)`: Database persistence for calculations
  - `save_market_states(symbol: str, interval_type: str, states: Sequence[StateSeries]) -> int`
  - `get_market_states(symbol: str, interval_type: str, start_time: datetime | None, end_time: datetime | None, limit: int) -> List[StateSeries]`
  - `save_pattern_events(symbol: str, interval_type: str, patterns: Sequence[PatternEvent]) -> int`
  - `get_pattern_events(symbol: str, interval_type: str, pattern_type: PatternType | None, start_time: datetime | None, end_time: datetime | None, limit: int) -> List[PatternEvent]`
  - `save_multi_timeframe_analysis(symbol: str, analysis: MultiTimeframeAnalysis) -> int`
  - `get_latest_multi_timeframe_analysis(symbol: str, htf_interval: str, trading_interval: str) -> dict | None`

**dgas/db/connection_pool.py**
- `PooledConnectionManager(settings: Settings | None)`: Manages PostgreSQL connection pool
  - `initialize_pool(min_size: int, max_size: int, max_inactive_connection_lifetime: float) -> None`
  - `get_pool() -> ConnectionPool`: Get pool instance
  - `get_connection() -> Generator[psycopg.Connection, None, None]`: Context manager for pooled connection
  - `initialize_async_pool(...) -> None`: Initialize async pool
  - `get_async_pool() -> AsyncConnectionPool`: Get async pool
  - `close_pool() -> None`: Close pool
  - `get_stats() -> dict[str, int]`: Pool statistics
- `get_pool_manager() -> PooledConnectionManager`: Global singleton
- `get_db_connection() -> Generator[psycopg.Connection, None, None]`: Convenience function

**dgas/db/enhanced_persistence.py**
- `OptimizedPredictionPersistence(settings: Settings | None)`: Enhanced persistence with caching and profiling
  - Extends `PredictionPersistence` with connection pooling, query caching, performance monitoring
  - `get_recent_runs(limit: int, status: str | None, use_cache: bool, cache_ttl_seconds: int) -> List[dict]`
  - `get_recent_signals(symbol: str | None, lookback_hours: int, min_confidence: float | None, limit: int, use_cache: bool, cache_ttl_seconds: int) -> List[dict]`
  - `get_metrics(metric_type: str | None, lookback_hours: int, aggregation_period: str | None, limit: int, use_cache: bool, cache_ttl_seconds: int) -> List[dict]`
  - `_execute_with_profiling(query_name: str, query_func: callable) -> Any`: Profile query execution

**dgas/db/query_cache.py**
- `CacheEntry(result: Any, timestamp: float, ttl_seconds: int, hit_count: int)`: Cached query result
- `QueryCache(max_size: int)`: In-memory query result cache
  - `get(query: str, params: tuple) -> Any | None`: Get cached result
  - `set(query: str, params: tuple, result: Any, ttl_seconds: int) -> None`: Cache result
  - `invalidate(query: str, params: tuple) -> None`: Remove entry
  - `clear() -> int`: Clear all entries
  - `get_stats() -> dict[str, Any]`: Cache statistics
- `CacheManager`: Manages multiple named caches
- `get_cache_manager() -> CacheManager`: Global singleton

**dgas/db/performance_monitor.py**
- `QueryMetrics`: Query performance metrics (query, execution_time_ms, row_count, success, error_message)
- `PerformanceMonitor`: Tracks query performance
  - `record_query(query: str, execution_time_ms: float, row_count: int, success: bool, error_message: str | None) -> None`
  - `get_summary(lookback_minutes: int) -> dict[str, Any]`: Aggregate statistics
  - `get_slow_queries(limit: int) -> List[QueryMetrics]`: Find slowest queries
- `profile_query(monitor: PerformanceMonitor, query_name: str)`: Decorator for query profiling
- `get_performance_monitor() -> PerformanceMonitor`: Global singleton

**dgas/db/optimizer.py**
- `DatabaseOptimizer`: Database query optimization utilities
- `get_optimizer() -> DatabaseOptimizer`: Global singleton

**Migrations**: 001_initial_schema.sql, 002_enhanced_states_patterns.sql, 003_prediction_system.sql, 004_exchange_calendar.sql

---

## Data Ingestion & Quality

**dgas/data/models.py**
- `IntervalData(BaseModel)`: Immutable OHLCV bar
  - Fields: symbol, exchange, timestamp (UTC), interval, open/high/low/close (Decimal), adjusted_close, volume (int)
  - Factories: `from_api_record(record: Dict, interval: str, symbol_override: str | None) -> IntervalData`, `from_api_list(records: Iterable[Dict], interval: str, symbol_override: str | None) -> List[IntervalData]`

**dgas/data/errors.py**
- Exceptions: `EODHDError` (base), `EODHDAuthError`, `EODHDRateLimitError`, `EODHDRequestError`, `EODHDParsingError`

**dgas/data/rate_limiter.py**
- `RateLimiter(max_calls: int, period: float, now_func: Callable | None, sleep_func: Callable | None)`
  - `acquire() -> None`: Block until call permitted (token bucket)

**dgas/data/client.py**
- `EODHDConfig(api_token, base_url, requests_per_minute, timeout, max_retries, session)`
  - Factory: `from_settings(Settings) -> EODHDConfig`
- `EODHDClient(config: EODHDConfig)`
  - `fetch_intraday(symbol: str, start: str | None, end: str | None, interval: str, limit: int) -> List[IntervalData]`
  - `fetch_eod(symbol: str, start: str | None, end: str | None) -> List[IntervalData]`
  - `list_exchange_symbols(exchange: str) -> List[Dict]`
  - `close() -> None`

**dgas/data/quality.py**
- `DataQualityReport`: Validate completeness, chronology, duplicates
- `analyze_intervals(Sequence[IntervalData]) -> DataQualityReport`: Validate data quality
- `summarize_reports(Iterable[DataQualityReport]) -> dict`: Aggregate quality metrics

**dgas/data/repository.py**
- `ensure_market_symbol(conn: Connection, symbol: str, exchange: str, **metadata) -> int`: Upsert symbol, return symbol_id
- `bulk_upsert_market_data(conn: Connection, symbol_id: int, interval: str, data: Sequence[IntervalData]) -> int`: Bulk upsert OHLCV bars
- `get_latest_timestamp(conn: Connection, symbol_id: int, interval: str) -> datetime | None`: Most recent timestamp
- `ensure_symbols_bulk(conn: Connection, symbols: Iterable[tuple[str, str]]) -> dict[str, int]`: Batch symbol creation
- `get_symbol_id(conn: Connection, symbol: str) -> int | None`: Look up registered symbols
- `fetch_market_data(conn: Connection, symbol: str, interval: str, *, start: datetime | None = None, end: datetime | None = None, limit: int | None = None) -> list[IntervalData]`: Chronological OHLCV retrieval

**dgas/data/ingestion.py**
- `IngestionSummary`: Symbol, interval, fetched, stored, quality, start, end
- `backfill_intraday(symbol: str, *, exchange: str, start_date: str, end_date: str, interval: str, client: EODHDClient | None) -> IngestionSummary`
- `incremental_update_intraday(symbol: str, *, exchange: str, interval: str, buffer_days: int, default_start: str | None, client: EODHDClient | None) -> IngestionSummary`
- `backfill_many(symbols: Sequence[tuple[str, str]], *, start_date: str, end_date: str, interval: str, client: EODHDClient | None) -> List[IngestionSummary]`

**dgas/data/exchange_calendar.py**
- `ExchangeCalendar(settings, use_cache)`: EODHD exchange API integration
  - `fetch_exchange_details(exchange_code: str, from_date: str, to_date: str) -> dict`: Fetch exchange metadata
  - `sync_exchange_calendar(exchange_code: str, force_refresh: bool) -> None`: Sync calendar to database
  - `is_trading_day(exchange_code: str, check_date: datetime) -> bool`: Check if date is trading day
  - `get_trading_hours(exchange_code: str, check_date: datetime) -> dict`: Get market open/close times

---

## Drummond Geometry Calculations

**dgas/calculations/pldot.py**
- `PLDotSeries`: Frozen dataclass (timestamp, value, projected_timestamp, projected_value, slope, displacement)
- `PLDotCalculator(displacement: int = 1)`
  - `from_intervals(Sequence[IntervalData]) -> List[PLDotSeries]`: 3-period MA of (high+low+close)/3, projected forward

**dgas/calculations/envelopes.py**
- `EnvelopeSeries`: Frozen dataclass (timestamp, center, upper, lower, width, position, method)
- `EnvelopeCalculator(method: str = "pldot_range", period: int = 3, multiplier: float = 1.5, percent: float = 0.02)`
  - Methods: "pldot_range" (Drummond 3-period std), "atr" (ATR-based), "percentage" (fixed %)
  - `from_intervals(intervals: Sequence[IntervalData], pldot: Sequence[PLDotSeries]) -> List[EnvelopeSeries]`

**dgas/calculations/drummond_lines.py**
- `DrummondLine`: Frozen dataclass (start_timestamp, end_timestamp, start_price, end_price, projected_timestamp, projected_price, slope, line_type)
- `DrummondZone`: Frozen dataclass (center_price, lower_price, upper_price, line_type, strength)
- `DrummondLineCalculator(projection_gap: int = 1)`
  - `from_intervals(Sequence[IntervalData]) -> List[DrummondLine]`: Generate support/resistance lines per two-bar pairs
- `aggregate_zones(lines: Iterable[DrummondLine], tolerance: float = 0.5) -> List[DrummondZone]`: Cluster nearby lines

**dgas/calculations/states.py**
- `MarketState(Enum)`: TREND, CONGESTION_ENTRANCE, CONGESTION_ACTION, CONGESTION_EXIT, REVERSAL
- `TrendDirection(Enum)`: UP, DOWN, NEUTRAL
- `StateSeries`: Frozen dataclass (timestamp, state, trend_direction, bars_in_state, previous_state, pldot_slope_trend, confidence, state_change_reason)
- `MarketStateClassifier(slope_threshold: float = 0.0001)`
  - `classify(intervals: Sequence[IntervalData], pldot_series: Sequence[PLDotSeries]) -> List[StateSeries]`: Applies Drummond 3-bar rule

**dgas/calculations/patterns.py**
- `PatternType(Enum)`: PLDOT_PUSH, PLDOT_REFRESH, EXHAUST, C_WAVE, CONGESTION_OSCILLATION
- `PatternEvent`: Frozen dataclass (pattern_type, direction, start_timestamp, end_timestamp, strength)
- `detect_pldot_push(intervals: Sequence[IntervalData], pldot: Sequence[PLDotSeries]) -> List[PatternEvent]`: 3+ bars where close and PLdot slope align
- `detect_pldot_refresh(intervals: Sequence[IntervalData], pldot: Sequence[PLDotSeries], tolerance: float = 0.1) -> List[PatternEvent]`: Price far from PLdot then returns
- `detect_exhaust(intervals: Sequence[IntervalData], pldot: Sequence[PLDotSeries], envelopes: Sequence[EnvelopeSeries], extension_threshold: float = 2.0) -> List[PatternEvent]`: Price extends beyond envelope then reverses
- `detect_c_wave(envelopes: Sequence[EnvelopeSeries], pldot: Sequence[PLDotSeries] | None, intervals: Sequence[IntervalData] | None) -> List[PatternEvent]`: 3+ bars at envelope extremes
- `detect_congestion_oscillation(envelopes: Sequence[EnvelopeSeries]) -> List[PatternEvent]`: 4+ bars oscillating in middle envelope region

**dgas/calculations/multi_timeframe.py**
- `TimeframeType(Enum)`: HIGHER, TRADING, LOWER
- `TimeframeData`: Frozen dataclass (timeframe, classification, pldot_series, envelope_series, state_series, pattern_events, drummond_zones)
- `PLDotOverlay`: Frozen dataclass (timestamp, htf_timeframe, htf_pldot_value, htf_slope, ltf_timeframe, ltf_pldot_value, distance_percent, position)
- `ConfluenceZone`: Frozen dataclass (level, upper_bound, lower_bound, strength, timeframes, zone_type, first_touch, last_touch, weighted_strength, sources, volatility)
- `TimeframeAlignment`: Frozen dataclass (timestamp, htf_state, htf_direction, htf_confidence, trading_tf_state, trading_tf_direction, trading_tf_confidence, alignment_score, alignment_type, trade_permitted)
- `MultiTimeframeAnalysis`: Frozen dataclass (timestamp, htf_timeframe, trading_timeframe, ltf_timeframe, htf_trend, htf_trend_strength, trading_tf_trend, alignment, pldot_overlay, confluence_zones, htf_patterns, trading_tf_patterns, pattern_confluence, signal_strength, risk_level, recommended_action)
- `MultiTimeframeCoordinator(htf_timeframe: str, trading_timeframe: str, ltf_timeframe: str | None, confluence_tolerance_pct: float = 0.5, alignment_threshold: float = 0.6)`
  - `analyze(htf_data: TimeframeData, trading_tf_data: TimeframeData, ltf_data: TimeframeData | None, target_timestamp: datetime | None) -> MultiTimeframeAnalysis`

**dgas/calculations/timeframe_builder.py**
- `build_timeframe_data(intervals: Sequence[IntervalData], timeframe: str, classification: TimeframeType) -> TimeframeData`: Construct complete TimeframeData with all indicators

**dgas/calculations/cache.py**
- `CacheKey`: Cache key representation (calculation_type, symbol, timeframe, parameters, data_hash)
- `CachedResult`: Cached calculation result (result, timestamp, ttl_seconds, hit_count, computation_time_ms)
- `CalculationCache(max_size: int, default_ttl_seconds: int)`: Specialized cache for calculations
  - `get(key: CacheKey) -> Any | None`, `set(key: CacheKey, result: Any, ttl_seconds: int | None) -> None`, `invalidate(key: CacheKey) -> None`, `clear_expired() -> int`, `get_stats() -> dict[str, Any]`
- `get_calculation_cache() -> CalculationCache`: Global singleton
- `CachedPLDotCalculator`, `CachedEnvelopeCalculator`: Cached calculator wrappers

**dgas/calculations/cache_manager.py**
- `InvalidationRule`: Cache invalidation rule (pattern, trigger, ttl_seconds, max_entries)
- `CacheInvalidationManager(cache: CalculationCache | None)`: Manages intelligent cache invalidation
  - `add_rule(rule: InvalidationRule) -> None`, `invalidate_by_pattern(pattern: str) -> int`, `register_data_update(symbol: str, timeframe: str) -> None`
- `DataUpdateListener`: Listener for data updates
- `get_invalidation_manager() -> CacheInvalidationManager`: Global singleton

**dgas/calculations/profiler.py**
- `CalculationMetrics`: Calculation metrics (calculation_type, symbol, timeframe, execution_time_ms, success, timestamp, cache_hit)
- `CalculationProfiler`: Profile Drummond geometry calculations
  - `record_calculation(...) -> None`, `get_summary() -> dict[str, Any]`, `get_slow_calculations(limit: int) -> List[CalculationMetrics]`
- `get_calculation_profiler() -> CalculationProfiler`: Global singleton

**dgas/calculations/optimized_coordinator.py**
- `OptimizedTimeframeData`: TimeframeData with pre-computed indexes for fast lookups
  - `get_state_at_timestamp(timestamp: datetime) -> StateSeries | None`: Binary search lookup
  - `get_pldot_at_timestamp(timestamp: datetime) -> PLDotSeries | None`: Binary search lookup
  - `get_envelope_at_timestamp(timestamp: datetime) -> EnvelopeSeries | None`: Binary search lookup
- `OptimizedMultiTimeframeCoordinator`: Performance-optimized coordinator with caching, binary search, memoization
  - `analyze(...) -> MultiTimeframeAnalysis`: Optimized analysis with profiling
  - `clear_cache() -> None`: Clear analysis cache

**dgas/calculations/benchmarks.py**
- `BenchmarkResult`: Benchmark result (name, execution_time_ms, memory_mb, success, error)
- `BenchmarkSuite`: Collection of benchmarks
- `BenchmarkRunner`: Run benchmarks and collect results
- `run_standard_benchmarks() -> Dict[str, Any]`: Run standard benchmark suite

---

## Backtesting Engine

**dgas/backtesting/entities.py**
- `PositionSide(Enum)`: LONG, SHORT
- `SignalAction(Enum)`: ENTER_LONG, ENTER_SHORT, EXIT_LONG, EXIT_SHORT, HOLD
- `Signal`: Dataclass (symbol, action, timestamp, price, metadata)
- `Position`: Dataclass (symbol, side, quantity, entry_price, entry_time, entry_commission, stop_loss, take_profit, confidence, notes)
- `Trade`: Dataclass (symbol, side, quantity, entry_time, exit_time, entry_price, exit_price, gross_profit, net_profit, commission_paid)
- `PortfolioSnapshot`: Dataclass (timestamp, equity, cash)
- `SimulationConfig`: Dataclass (initial_capital, commission_rate, slippage_bps, max_positions, risk_per_trade_pct)
- `BacktestResult`: Dataclass (config, symbol, trades, equity_curve, starting_capital, ending_capital, start_date, end_date, total_bars, metadata)

**dgas/backtesting/data_loader.py**
- `BacktestBar`: Dataclass (timestamp, open, high, low, close, volume)
- `BacktestDataset`: Dataclass (symbol, interval, bars, indicator_snapshots)
- `load_ohlcv(symbol, interval, *, start=None, end=None, limit=None, conn=None) -> list[IntervalData]`: Fetch chronological OHLCV bars
- `load_dataset(symbol, interval, *, start=None, end=None, include_indicators=True, limit=None, conn=None) -> BacktestDataset`: Bundle bars plus optional indicator snapshots

**dgas/backtesting/engine.py**
- `SimulationEngine(SimulationConfig | None)`: Deterministic execution core
  - `run(dataset: BacktestDataset, strategy: BaseStrategy) -> BacktestResult`: Execute strategy signals bar-by-bar

**dgas/backtesting/portfolio_data_loader.py**
- `SymbolDataBundle`: Frozen dataclass (symbol, bars, bar_count)
- `PortfolioTimestep`: Frozen dataclass (timestamp, bars: dict[str, IntervalData], symbols_present: set[str])
- `PortfolioDataLoader(regular_hours_only: bool, exchange_code: str)`: Load and synchronize market data for portfolio-level backtesting
  - `load_portfolio_data(symbols: Sequence[str], interval: str, start: datetime | None, end: datetime | None) -> dict[str, SymbolDataBundle]`: Load data for all symbols
  - `create_synchronized_timeline(bundles: dict[str, SymbolDataBundle]) -> List[PortfolioTimestep]`: Create synchronized timeline across symbols
  - `get_data_summary(bundles: dict[str, SymbolDataBundle]) -> dict`: Get summary statistics

**dgas/backtesting/portfolio_engine.py**
- `PortfolioBacktestConfig`: Dataclass (initial_capital, risk_per_trade_pct, max_positions, max_portfolio_risk_pct, commission_rate, slippage_bps, regular_hours_only, exchange_code, max_signals_per_bar, allow_short, htf_interval, trading_interval, min_signal_confidence, confidence_scaling_enabled)
- `PortfolioBacktestResult`: Dataclass (config, symbols, trades, equity_curve, starting_capital, ending_capital, ending_equity, start_date, end_date, total_bars, metadata)
- `PortfolioBacktestEngine(config: PortfolioBacktestConfig | None, strategy: BaseStrategy | None)`: Main portfolio backtesting engine
  - `run(symbols: List[str], interval: str, start: datetime | None, end: datetime | None) -> PortfolioBacktestResult`: Run portfolio backtest across multiple symbols
  - `_process_timestep(timestep: PortfolioTimestep, idx: int, total: int) -> None`: Process single timestamp across all symbols
  - `_generate_entry_signals(timestep: PortfolioTimestep, portfolio_state: PortfolioState) -> List[RankedSignal]`: Generate entry signals for all symbols
  - `_execute_entry_signal(ranked_signal: RankedSignal, timestep: PortfolioTimestep) -> None`: Execute entry signal
  - `_check_exits(timestep: PortfolioTimestep, current_prices: Dict[str, Decimal]) -> None`: Check and execute exits on existing positions

**dgas/backtesting/portfolio_position_manager.py**
- `PortfolioPosition`: Dataclass (position: Position, risk_amount: Decimal, stop_loss: Decimal | None, target: Decimal | None, max_adverse_excursion: Decimal, max_favorable_excursion: Decimal, metadata: Dict)
- `PortfolioState`: Dataclass (timestamp, cash, positions: Dict[str, PortfolioPosition], total_equity, total_position_value, available_capital)
- `PortfolioPositionManager(initial_capital, max_positions, max_portfolio_risk_pct, risk_per_trade_pct, commission_rate, slippage_bps)`: Manage positions across multiple symbols with shared capital
  - `get_current_state(timestamp: datetime, prices: Dict[str, Decimal]) -> PortfolioState`: Get current portfolio state
  - `can_open_position(symbol: str, risk_amount: Decimal) -> tuple[bool, str | None]`: Check if new position can be opened
  - `calculate_position_size(symbol: str, entry_price: Decimal, stop_loss: Decimal, direction: int, current_prices: Dict[str, Decimal] | None) -> tuple[Decimal, Decimal]`: Calculate position size based on portfolio risk
  - `open_position(symbol: str, side: PositionSide, quantity: Decimal, entry_price: Decimal, entry_time: datetime, stop_loss: Decimal | None, target: Decimal | None, metadata: Dict) -> PortfolioPosition`: Open new position
  - `close_position(symbol: str, exit_price: Decimal, exit_time: datetime) -> Trade`: Close existing position
  - `update_positions(timestamp: datetime, prices: Dict[str, Decimal]) -> None`: Update all positions with current prices

**dgas/backtesting/portfolio_indicator_calculator.py**
- `HTFDataCache`: Dataclass (symbol, interval, bars, start_date, end_date)
- `PortfolioIndicatorCalculator(htf_interval: str, trading_interval: str)`: Calculate indicators on-the-fly for portfolio backtesting
  - `load_htf_data_for_symbol(symbol: str, start: datetime, end: datetime) -> None`: Pre-load higher timeframe data
  - `calculate_indicators(symbol: str, current_bar: IntervalData, historical_bars: Sequence[IntervalData]) -> Dict[str, Any]`: Calculate multi-timeframe indicators
  - `preload_htf_data_for_portfolio(symbols: List[str], start: datetime, end: datetime) -> None`: Pre-load HTF data for all symbols
  - `get_cache_stats() -> Dict[str, Any]`: Get statistics about cached HTF data

**dgas/backtesting/signal_ranker.py**
- `RankingCriteria(Enum)`: SIGNAL_STRENGTH, RISK_REWARD_RATIO, CONFLUENCE, TREND_ALIGNMENT, VOLATILITY
- `RankedSignal`: Dataclass (symbol, signal, score, entry_price, stop_loss, target, risk_amount, metadata)
- `SignalRanker(criteria_weights, min_risk_reward, max_correlated_positions)`: Rank and prioritize trading signals
  - `rank_signals(signals: List[RankedSignal], existing_positions: Dict[str, Any] | None) -> List[RankedSignal]`: Rank signals by composite score
  - `select_top_signals(ranked_signals: List[RankedSignal], max_signals: int, min_score: Decimal | None) -> List[RankedSignal]`: Select top N signals
  - `group_by_direction(signals: List[RankedSignal]) -> Dict[str, List[RankedSignal]]`: Group signals by direction

**dgas/backtesting/signal_evaluator.py**
- `SignalPrediction`: Dataclass (signal_id, symbol, signal_timestamp, signal_type, predicted_entry, predicted_stop_loss, predicted_target, confidence, signal_strength)
- `SignalOutcome`: Dataclass (signal_id, actual_entry, actual_exit, actual_entry_time, actual_exit_time, trade_executed, trade_id, pnl, hit_stop_loss, hit_target, exit_reason)
- `SignalAccuracyMetrics`: Dataclass (total_signals, executed_signals, winning_signals, losing_signals, win_rate, avg_confidence_winning, avg_confidence_losing, avg_confidence_all, signals_by_type, win_rate_by_type, win_rate_by_confidence_bucket)
- `SignalEvaluator()`: Evaluate signal accuracy by comparing predictions to actual outcomes
  - `register_signal(signal: GeneratedSignal) -> None`: Register a predicted signal
  - `register_trade(trade: Trade, signal_id: str | None) -> None`: Register a completed trade and match it to a signal
  - `calculate_metrics() -> SignalAccuracyMetrics`: Calculate accuracy metrics

**dgas/backtesting/execution/trade_executor.py**
- `TradeExecutionResult`: Dataclass (trade, new_position, cash_change, commission_paid)
- `BaseTradeExecutor(commission_rate, slippage_bps)`: Base class for trade execution with shared logic
  - `apply_slippage(price: Decimal, side: PositionSide, *, is_entry: bool) -> Decimal`: Apply slippage to a price
  - `compute_commission(quantity: Decimal, price: Decimal) -> Decimal`: Calculate commission
  - `calculate_gross_profit(position: Position, exit_price: Decimal) -> Decimal`: Calculate gross profit/loss
  - `calculate_net_profit(position: Position, exit_price: Decimal, exit_commission: Decimal) -> Decimal`: Calculate net profit/loss after commissions
  - `normalize_quantity(quantity: Decimal) -> Decimal`: Normalize quantity to valid step size
  - `open_position(symbol: str, side: PositionSide, quantity: Decimal, entry_price: Decimal, entry_time: datetime, metadata: dict | None, stop_loss: Decimal | None, take_profit: Decimal | None) -> tuple[Position, Decimal]`: Open a new position
  - `close_position(position: Position, exit_price: Decimal, exit_time: datetime) -> Trade`: Close an existing position

**dgas/backtesting/indicator_loader.py**
- `load_indicators_from_db(symbol: str, interval: str, timestamp: datetime, conn: Connection | None) -> dict[str, Any]`: Load pre-computed indicators from database
- `load_indicators_batch(symbols: List[str], interval: str, timestamps: List[datetime], conn: Connection | None) -> Dict[str, Dict[datetime, dict]]`: Load indicators for multiple symbols/timestamps

**dgas/backtesting/strategies/base.py**
- `StrategyConfig`: Pydantic base model for strategy configuration
- `StrategyContext`: Dataclass (symbol, bar, position, cash, equity, indicators, history)
- `BaseStrategy`: Abstract base class for trading strategies
  - `prepare(bars: Sequence[IntervalData]) -> None`: Prepare strategy with historical data
  - `on_bar(context: StrategyContext) -> Iterable[Signal]`: Generate signals for current bar
- `rolling_history() -> deque`: Helper function for rolling history

**dgas/backtesting/strategies/multi_timeframe.py**
- `MultiTimeframeStrategyConfig(StrategyConfig)`: Configuration for multi-timeframe strategy
- `MultiTimeframeStrategy(BaseStrategy)`: Multi-timeframe strategy implementation

**dgas/backtesting/strategies/prediction_signal.py**
- `PredictionSignalStrategyConfig(StrategyConfig)`: Configuration for prediction signal strategy
- `PredictionSignalStrategy(BaseStrategy)`: Strategy that uses PredictionEngine's SignalGenerator for signal generation
  - `_get_signal_generator(htf_timeframe: str, trading_timeframe: str) -> SignalGenerator`: Get or create SignalGenerator

**dgas/backtesting/strategies/registry.py**
- `STRATEGY_REGISTRY`: Mapping names to classes
- `instantiate_strategy(name: str, params: Mapping[str, Any] | None) -> BaseStrategy`: Build configured strategies from CLI overrides

**dgas/backtesting/runner.py**
- `BacktestRequest`: Dataclass encapsulating CLI options
- `BacktestRunResult`: Dataclass (request, result, performance, symbol)
- `BacktestRunner`: High-level orchestration
  - `run(request: BacktestRequest) -> list[BacktestRunResult]`: Load datasets, instantiate strategy, run engine, compute metrics, optionally persist

**dgas/backtesting/metrics.py**
- `PerformanceSummary`: Dataclass with performance metrics
- `calculate_performance(result: BacktestResult, risk_free_rate: Decimal) -> PerformanceSummary`: Compute total/annualized return, volatility, Sharpe, Sortino, max drawdown, trade stats, profit factor, net profit

**dgas/backtesting/persistence.py**
- `persist_backtest(result: BacktestResult, performance: PerformanceSummary, metadata: dict | None, conn: Connection | None) -> int`: Store backtest summary and trade ledger in PostgreSQL

**dgas/backtesting/reporting.py**
- `build_summary_table(run_results: List[BacktestRunResult]) -> rich.table.Table`: Console-ready summary view
- `export_markdown(run: BacktestRunResult, path: Path) -> None`, `export_json(run: BacktestRunResult, path: Path) -> None`: Generate shareable artifacts

---

## Prediction System

**dgas/prediction/persistence.py**
- `PredictionPersistence(settings: Settings | None)`: Database persistence for prediction system
  - `save_prediction_run(...) -> int`: Save prediction cycle metadata with timing breakdown
  - `get_recent_runs(limit: int, status: str | None) -> List[dict]`: Retrieve recent prediction runs
  - `save_generated_signals(run_id: int, signals: Sequence[GeneratedSignal]) -> int`: Bulk save trading signals
  - `get_recent_signals(symbol: str | None, lookback_hours: int, min_confidence: float | None, limit: int) -> List[dict]`: Query signals with filters
  - `update_signal_outcome(signal_id: int, outcome: str, actual_high: Decimal, actual_low: Decimal, actual_close: Decimal, pnl_pct: float) -> None`: Update signal with actual price data
  - `save_metric(metric_type: str, metric_value: float, aggregation_period: str | None, metadata: dict | None) -> int`: Save performance/calibration metric
  - `get_metrics(metric_type: str | None, lookback_hours: int, aggregation_period: str | None, limit: int) -> List[dict]`: Query metrics for analysis
  - `update_scheduler_state(...) -> None`: Update singleton scheduler state
  - `get_scheduler_state() -> dict | None`: Retrieve current scheduler status

**dgas/prediction/engine.py**
- `SignalType(Enum)`: LONG, SHORT, EXIT_LONG, EXIT_SHORT
- `GeneratedSignal`: Complete trading signal with context (symbol, signal_timestamp, signal_type, entry_price, stop_loss, target_price, confidence, signal_strength, timeframe_alignment, risk_reward_ratio, htf_trend, trading_tf_state, confluence_zones_count, pattern_context, notification tracking, outcome tracking)
- `SignalGenerator(coordinator, min_alignment_score, min_signal_strength, stop_loss_atr_multiplier, target_rr_ratio)`: Signal generation from multi-timeframe analysis
  - `generate_signals(symbol: str, htf_data: TimeframeData, trading_tf_data: TimeframeData, ltf_data: TimeframeData | None) -> List[GeneratedSignal]`: Create actionable signals
- `SignalAggregator(dedup_time_window_minutes, min_confidence, min_alignment)`: De-duplication and ranking
  - `aggregate_signals(signals: List[GeneratedSignal], min_confidence, min_alignment, enabled_patterns, max_signals) -> List[GeneratedSignal]`: Filter, rank, deduplicate
- `PredictionRunResult`: Execution metrics from prediction cycle
- `PredictionEngine(settings, persistence, signal_generator, lookback_bars: int)`: Prediction pipeline orchestration
  - `execute_prediction_cycle(symbols: List[str], interval: str, timeframes: dict, htf_interval: str, trading_interval: str, persist_results: bool) -> PredictionRunResult`: Full cycle

**dgas/prediction/scheduler.py**
- `TradingSession`: Trading session configuration (market_open, market_close, timezone, trading_days)
- `SchedulerConfig`: Scheduler configuration (interval, exchange_code, catch_up_on_start, daemon_mode)
- `MarketHoursManager(exchange_code: str, session: TradingSession, calendar: ExchangeCalendar)`: Trading hours awareness
  - `is_market_open(dt: datetime) -> bool`: Check if market is currently open
  - `next_market_open(from_dt: datetime) -> datetime`: Calculate next market open time
  - `next_market_close(from_dt: datetime) -> datetime`: Calculate next market close time
- `PredictionScheduler(config: SchedulerConfig, engine: PredictionEngine, persistence: PredictionPersistence, market_hours: MarketHoursManager)`: APScheduler-based prediction orchestration
  - `start() -> None`: Start scheduler with APScheduler, run catch-up if enabled
  - `stop(wait: bool) -> None`: Graceful shutdown
  - `run_once() -> PredictionRunResult`: Execute single prediction cycle manually

**dgas/prediction/notifications/router.py**
- `NotificationConfig`: Configuration for notification delivery
- `NotificationAdapter(ABC)`: Abstract base for notification channel implementations
- `NotificationRouter(config: NotificationConfig, adapters: dict[str, NotificationAdapter])`: Routes signals to configured channels
  - `send_notifications(signals: List[GeneratedSignal], run_metadata: dict) -> None`: Send notifications through all enabled channels

**dgas/prediction/notifications/adapters/discord.py**
- `DiscordAdapter(bot_token: str, channel_id: str, rate_limit_delay: float, timeout: int)`: Discord Bot API integration
  - `send(signals: List[GeneratedSignal], metadata: dict) -> bool`: Send signals as individual Discord embeds

**dgas/prediction/notifications/adapters/console.py**
- `ConsoleAdapter(max_signals: int, output_format: str)`: Rich console table output
  - `send(signals: List[GeneratedSignal], metadata: dict) -> bool`: Display signals in formatted Rich table

**dgas/prediction/monitoring/performance.py**
- `LatencyMetrics`: Latency measurements (data_fetch_ms, indicator_calc_ms, signal_generation_ms, notification_ms, total_ms)
- `ThroughputMetrics`: Throughput measurements (symbols_processed, signals_generated, execution_time_ms, symbols_per_second)
- `PerformanceTracker(persistence: PredictionPersistence)`: System performance monitoring
  - `track_cycle(run_id: int, latency: LatencyMetrics, throughput: ThroughputMetrics, errors: List[str]) -> None`: Record cycle metrics
  - `get_performance_summary(lookback_hours: int) -> dict[str, Any]`: Aggregate statistics
  - `check_sla_compliance() -> dict[str, bool]`: Verify SLA targets

**dgas/prediction/monitoring/calibration.py**
- `CalibrationEngine(persistence: PredictionPersistence, evaluation_window_hours: int)`: Signal accuracy validation
  - `evaluate_signal(signal: GeneratedSignal, actual_prices: dict) -> dict`: Compare signal to actual outcome
  - `batch_evaluate(lookback_hours: int) -> dict[str, Any]`: Evaluate all pending signals
  - `get_calibration_report(date_range: tuple[datetime, datetime]) -> dict[str, Any]`: Win rate by confidence bucket

---

## CLI Commands

**dgas/cli/analyze.py**
- `load_market_data(symbol: str, interval: str, lookback_bars: int) -> list[IntervalData]`: Load from DB
- `calculate_indicators(intervals: list[IntervalData]) -> TimeframeData`: Calculate all indicators for single TF
- `display_single_timeframe_analysis(symbol: str, interval: str, tf_data: TimeframeData) -> None`: Rich console output
- `display_multi_timeframe_analysis(symbol: str, analysis: MultiTimeframeAnalysis) -> None`: Rich console output
- `run_analyze_command(symbols: list[str], htf_interval: str, trading_interval: str, lookback_bars: int, save_to_db: bool, output_format: str) -> int`: Full analysis workflow

**dgas/cli/backtest.py**
- `run_backtest_command(...) -> int`: Parse CLI arguments, invoke BacktestRunner, render Rich summary/detailed output, emit optional Markdown/JSON artifacts

**dgas/cli/configure.py**
- `setup_configure_parser(subparsers) -> ArgumentParser`: Set up configure subcommand
- `_init_command(args: Namespace) -> int`: Interactive configuration wizard
- `_show_command(args: Namespace) -> int`: Display current configuration
- `_validate_command(args: Namespace) -> int`: Validate configuration file
- `_edit_command(args: Namespace) -> int`: Edit configuration file

**dgas/cli/data.py**
- `setup_data_parser(subparsers) -> ArgumentParser`: Set up data subcommand
- Data management commands: backfill, update, report, quality

**dgas/cli/predict.py**
- `setup_predict_parser(subparsers) -> ArgumentParser`: Set up predict subcommand
- `run_predict_command(args: Namespace) -> int`: Generate trading signals with flexible output formats

**dgas/cli/report.py**
- `setup_report_parser(subparsers) -> ArgumentParser`: Set up report subcommand
- Report generation: performance, signals, backtests, system

**dgas/cli/scheduler_cli.py**
- `setup_scheduler_parser(subparsers) -> ArgumentParser`: Set up scheduler subcommand
- Scheduler management: start, stop, status, run-once

**dgas/cli/status_cli.py**
- `setup_status_parser(subparsers) -> ArgumentParser`: Set up status subcommand
- System health monitoring: database, API, scheduler, predictions

**dgas/cli/monitor.py**
- `setup_monitor_parser(subparsers) -> ArgumentParser`: Set up monitor subcommand
- Performance monitoring: latency, throughput, errors, SLA compliance

---

## Dashboard System

**dgas/dashboard/__main__.py**
- `build_parser() -> ArgumentParser`: Dashboard CLI argument parser
- `main(argv: list[str] | None) -> int`: Start Streamlit dashboard server

**dgas/dashboard/app.py**
- `configure_page() -> None`: Configure Streamlit page settings
- `render_sidebar() -> str`: Render sidebar navigation and return selected page
- `render_page(page: str) -> None`: Render selected page
- `main() -> NoReturn`: Main application entry point

**dgas/dashboard/pages/**: 01_Overview.py, 02_Data.py, 03_Predictions.py, 04_Backtests.py, 05_System_Status.py, 06_Custom_Dashboard.py
- Each page has `render() -> None` function

**dgas/dashboard/components/**: database.py, charts.py, notifications.py, utils.py
- Database query components, chart components, notification UI, shared utilities

**dgas/dashboard/widgets/**: base.py, metric.py, chart.py, table.py
- Base widget class and implementations (Metric, Chart, Table widgets)

**dgas/dashboard/layout/manager.py**
- `LayoutManager`: Layout management for custom dashboards
- `get_manager() -> LayoutManager`: Global singleton

**dgas/dashboard/filters/**: preset_manager.py, preset_ui.py
- Filter preset management and UI

**dgas/dashboard/export/enhanced_exporter.py**
- `EnhancedExporter`: Multi-format export (CSV, Excel, JSON, PDF)

**dgas/dashboard/services/notification_service.py**
- `NotificationService`: Notification service integration

**dgas/dashboard/realtime_client.py**
- `RealtimeClient`: Real-time data updates via WebSocket
- `get_client() -> RealtimeClient`: Get real-time client instance
- `setup_realtime_client() -> None`: Initialize real-time client

**dgas/dashboard/websocket_server.py**
- `DashboardWebSocketServer`: WebSocket server for real-time updates
- `start_server(host: str, port: int) -> DashboardWebSocketServer`: Start WebSocket server

**dgas/dashboard/performance/optimizer.py**
- `PerformanceMonitor`: Dashboard performance optimization
- `CacheManager`: Query result caching for dashboard
- `LazyLoader`: Lazy loading for dashboard components

**dgas/dashboard/utils/alert_rules.py**
- `AlertRulesEngine`: Alert rule management

---

## Monitoring

**dgas/monitoring/report.py**
- `SymbolIngestionStats`: Symbol ingestion statistics
- `generate_ingestion_report(interval: str) -> List[SymbolIngestionStats]`: Query DB for coverage
- `render_markdown_report(stats: Iterable[SymbolIngestionStats]) -> str`: Format as GFM table
- `write_report(stats: Iterable[SymbolIngestionStats], output_path: Path) -> None`

---

## File Connection Map (ASCII)

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                              CLI Entry Point                                │
│  dgas/__main__.py                                                           │
│      ├─ analyze      → cli/analyze.py → calculations/, data/                │
│      ├─ backtest     → cli/backtest.py → backtesting/ (runner, engine)      │
│      ├─ configure    → cli/configure.py → config/ (loader, schema)        │
│      ├─ data         → cli/data.py → data/ (ingestion, repository)          │
│      ├─ predict      → cli/predict.py → prediction/ (engine, scheduler)    │
│      ├─ report       → cli/report.py → monitoring/, prediction/             │
│      ├─ scheduler    → cli/scheduler_cli.py → prediction/scheduler.py       │
│      ├─ status       → cli/status_cli.py → db/, prediction/                 │
│      ├─ monitor      → cli/monitor.py → prediction/monitoring/              │
│      └─ data-report  → monitoring/report.py                                 │
└──────────────────────┬────────────────────────────────────────────────────────┘
                       │
                       ↓
┌─────────────────────────────────────────────────────────────────────────────┐
│                        Configuration Layer                                   │
│  config/loader.py → config/schema.py → config/adapter.py                    │
│  settings.py (legacy)                                                        │
│  core/config_manager.py (server resource config)                              │
└──────────────────────┬────────────────────────────────────────────────────────┘
                       │
        ┌───────────────┴──────────────┬──────────────────┬─────────────────────┐
        ↓                              ↓                  ↓                     ↓
┌───────────────┐   ┌────────────────────┐   ┌────────────────┐   ┌────────────────────┐
│  Database     │   │ Data Ingestion     │   │ Calculations    │   │ Backtesting         │
│  db/          │   │ data/              │   │ calculations/   │   │ backtesting/       │
│  - connection_pool.py                                                  │   │ - client, repo    │   │ - PLdot/States/    │   │ - runner, engine,  │
│  - enhanced_persistence.py                                              │   │ - quality,         │   │   Patterns/MTF    │   │   metrics,         │
│  - query_cache.py                                                       │   │   ingestion       │   │ - cache/profiler  │   │   persistence/     │
│  - performance_monitor.py                                               │   │ - exchange_calendar│   │ - optimized_*      │   │   reporting        │
│  - persistence.py                                                      │   │                   │   │ - timeframe_builder│   │ - strategies/      │
│  - migrations.py                                                        │   │                   │   │                   │   │   (base, registry) │
│  - optimizer.py                                                        │   │                   │   │                   │   │ - portfolio_*      │
│                                                                          │   │                   │   │                   │   │ - signal_*         │
│                                                                          │   │                   │   │                   │   │ - execution/       │
└──────┬────────┘   └─────────┬──────────┘   └────────┬───────┘   └────────┬──────────┘
       │                      │                      │                    │
       ↓                      ↓                      ↓                    ↓
PostgreSQL ← bulk_upsert_market_data()   Indicator outputs → strategies   persist_backtest()
Tables: market_data, market_states_v2, pattern_events, multi_timeframe_analysis, 
        prediction_runs, generated_signals, prediction_metrics, scheduler_state,
        backtest_results, backtest_trades, exchanges, market_holidays, trading_days

┌─────────────────────────────────────────────────────────────────────────────┐
│                        Prediction System                                     │
│  prediction/                                                                 │
│  - engine.py → SignalGenerator, SignalAggregator, PredictionEngine         │
│  - scheduler.py → PredictionScheduler, MarketHoursManager                   │
│  - persistence.py → PredictionPersistence                                   │
│  - notifications/router.py → NotificationRouter                              │
│  - notifications/adapters/ → DiscordAdapter, ConsoleAdapter                │
│  - monitoring/ → PerformanceTracker, CalibrationEngine                      │
└──────────────────────┬────────────────────────────────────────────────────────┘
                       │
                       ↓
┌─────────────────────────────────────────────────────────────────────────────┐
│                        Dashboard System                                      │
│  dashboard/                                                                  │
│  - app.py → Main Streamlit application                                      │
│  - pages/ → Overview, Data, Predictions, Backtests, SystemStatus, Custom    │
│  - components/ → Database, Charts, Notifications, Utils                     │
│  - widgets/ → Metric, Chart, Table widgets                                   │
│  - layout/manager.py → Layout persistence                                   │
│  - filters/ → Preset management                                             │
│  - export/ → Multi-format export                                            │
│  - realtime_client.py → WebSocket client                                     │
│  - websocket_server.py → WebSocket server                                    │
│  - performance/ → Performance optimization                                  │
│  - services/ → Notification service                                         │
│  - utils/ → Alert rules                                                      │
└─────────────────────────────────────────────────────────────────────────────┘

┌─────────────────────────────────────────────────────────────────────────────┐
│                        Core Utilities                                        │
│  core/                                                                       │
│  - config_manager.py → Server resource configuration                         │
│  - connection_pool.py → Database connection pooling                         │
│  - parallel_processor.py → Parallel batch processing                       │
│  - io_optimizer.py → Batch I/O, memory monitoring, file writing            │
│  utils/                                                                      │
│  - market_hours_filter.py → Regular hours filtering                         │
└─────────────────────────────────────────────────────────────────────────────┘
```

---

## Data Flow Summary

1. **Configuration**: `config/loader.py` loads YAML/JSON → `config/adapter.py` merges with env vars → `UnifiedSettings` provides unified interface
2. **CLI Routing**: `__main__.py` dispatches to subcommands (analyze, backtest, configure, data, predict, report, scheduler, status, monitor)
3. **Ingestion**: `data/` modules fetch EODHD data, validate quality, persist via `bulk_upsert_market_data()`
4. **Calculations**: `calculations/` modules produce PLdot, envelopes, states, patterns, multi-timeframe alignment (with caching and profiling)
5. **Backtesting**: 
   - Single-symbol: `backtesting/runner.py` loads historical bars, runs strategies through `engine.py`, computes metrics, optionally persists
   - Portfolio: `backtesting/portfolio_engine.py` coordinates multi-symbol backtesting with shared capital, signal ranking, position management
6. **Prediction**: `prediction/engine.py` orchestrates signal generation → `prediction/scheduler.py` schedules execution → `prediction/notifications/` delivers alerts
7. **Dashboard**: `dashboard/app.py` serves Streamlit UI → `dashboard/realtime_client.py` connects to WebSocket → pages render data from persistence layer
8. **Monitoring**: `monitoring/report.py` generates ingestion coverage summaries; `prediction/monitoring/` tracks performance and calibration

---

## Code Style & Conventions

### Type Safety
- All functions use type hints (enforced by mypy --disallow-untyped-defs)
- Pydantic models for validation (Settings, DGASConfig, IntervalData)
- Decimal for prices (avoid float rounding), datetime with timezone.utc
- Frozen dataclasses for immutable calculation results (PLDotSeries, EnvelopeSeries, StateSeries, etc.)

### Error Handling
- Custom exceptions (EODHDError hierarchy) for external API failures
- Context managers for resources (get_connection, get_db_connection, _client_context, DrummondPersistence)
- Transactional DB operations (commit on success, rollback on exception)

### Code Organization (DRY)
- Single Settings instance via @lru_cache
- UnifiedSettings adapter bridges new config system and legacy Settings
- Reusable repository functions (ensure_market_symbol, bulk_upsert_market_data)
- Shared quality analysis (analyze_intervals, summarize_reports)
- Factory methods for data models (IntervalData.from_api_record, IntervalData.from_api_list)
- Calculation caching (CalculationCache, CacheInvalidationManager)
- Query result caching (QueryCache, CacheManager)
- Connection pooling (PooledConnectionManager, DatabaseConnectionPool)
- Shared trade execution logic (BaseTradeExecutor) eliminates duplication between SimulationEngine and PortfolioPositionManager

### Naming
- snake_case for functions, variables, modules
- PascalCase for classes, Pydantic models, Enums
- Private helpers prefixed with _ (e.g., _ensure_migrations_table, _build_line)
- Descriptive names: bulk_upsert_market_data, not save_data; MultiTimeframeCoordinator, not MTF

### Dependencies
- Production: numpy, pandas, pydantic, pydantic-settings, python-dotenv, requests, sqlalchemy, psycopg[binary], structlog, rich, apscheduler, pyyaml, psutil
- Dashboard: streamlit, plotly
- Dev: pytest, ruff (linter/formatter), mypy (type checker)
- Line length: 100 chars (ruff config)

### Testing
- pytest with pythonpath = ["src"], testpaths = ["tests"]
- mypy excludes tests/ directory
- Coverage: calculations/, data/, monitoring/, prediction/, backtesting/ have tests

---

## Database Schema Notes

### Core Tables
- **market_symbols**: symbol (unique), exchange, metadata (sector, industry, market_cap)
- **market_data**: OHLCV bars with interval_type, unique on (symbol_id, timestamp, interval_type)
- **market_states_v2**: Enhanced state classification with confidence, trend direction, PLdot slope, bars_in_state
- **pattern_events**: Detected patterns (PLDOT_PUSH, EXHAUST, C_WAVE, etc.) with direction, strength, timestamps
- **multi_timeframe_analysis**: HTF/trading TF alignment, signal strength, recommended action, confluence zones count
- **confluence_zones**: Support/resistance zones confirmed by multiple timeframes (strength >= 2)
- **prediction_runs**: Prediction cycle metadata with timing breakdown
- **generated_signals**: Trading signals with confidence scores, context, outcomes, notification tracking
- **prediction_metrics**: Time-series performance and calibration metrics
- **scheduler_state**: Singleton table for scheduler status and recovery
- **exchanges**: Exchange metadata (timezone, market_open/close times, country, currency, sync tracking)
- **market_holidays**: Holiday and half-day schedules with early_close_time support
- **trading_days**: Pre-computed trading calendar with actual_close times for half-days
- **backtest_results**: Strategy metrics (Sharpe, max drawdown, win rate, etc.)
- **backtest_trades**: Individual trades per backtest with P&L

### Indexing Strategy
- DESC indexes on (symbol_id, timestamp) for recent data queries
- Multi-column indexes for state/pattern queries (symbol_id, state, timestamp DESC)
- Partial indexes for high-signal-strength analysis (WHERE signal_strength >= 0.6)
- Cascade deletes on symbol removal

---

## Veteran Developer Observations

### Architecture Strengths
1. **Clear separation of concerns**: data/, calculations/, db/, cli/, prediction/, dashboard/, backtesting/ are well-defined boundaries
2. **Type safety**: Comprehensive type hints and Pydantic validation catch errors early
3. **Immutable data structures**: Frozen dataclasses prevent accidental mutations
4. **DRY principle**: Reusable repository functions, shared quality checks, factory methods, unified configuration, shared trade execution logic
5. **Multi-timeframe first**: Core Drummond principle is properly implemented with HTF trend filtering
6. **Performance optimizations**: Connection pooling, query caching, calculation caching, profiling built-in, parallel processing support
7. **Portfolio-level backtesting**: Sophisticated multi-symbol backtesting with shared capital, signal ranking, position management
8. **Signal evaluation**: Built-in signal accuracy tracking and evaluation
9. **Unified configuration**: Seamless bridge between new config files and legacy environment variables
10. **Real-time capabilities**: WebSocket support for live dashboard updates
11. **Comprehensive monitoring**: Performance tracking, calibration, SLA compliance built-in

### Areas for Improvement
1. **Error handling**: Some areas could benefit from more specific exception types (e.g., CalculationError, PersistenceError)
2. **Logging**: Consider structured logging (structlog is in dependencies but not extensively used)
3. **Testing**: Some calculation modules could use more edge case coverage
4. **Documentation**: Inline docstrings are good, but some complex algorithms (multi-timeframe coordination, portfolio backtesting) could use more explanation
5. **Async support**: Consider async/await for I/O-bound operations (database, API calls) in future versions
6. **Distributed caching**: Current caching is in-memory; consider Redis for multi-process deployments

### Code Quality
- **Readability**: High - clear function names, good type hints, logical structure
- **Maintainability**: High - modular design, clear boundaries, easy to extend
- **Performance**: Excellent - connection pooling, caching, profiling, optimizations achieve <200ms target
- **Testability**: Good - functions are pure where possible, dependencies are injectable

### Data Model Consistency
- **Decimal precision**: Consistently used for all prices (good)
- **UTC timestamps**: Consistently applied (good)
- **Frozen models**: All calculation results are immutable (excellent)
- **Enum usage**: Consistent use of Enums for state types, pattern types, trend directions (good)

### Design Patterns
- **Factory pattern**: IntervalData.from_api_record, EODHDConfig.from_settings, ConfigLoader.generate_sample_config
- **Strategy pattern**: EnvelopeCalculator methods, NotificationAdapter implementations, BaseStrategy for backtesting
- **Context manager pattern**: get_connection, get_db_connection, _client_context, DrummondPersistence
- **Repository pattern**: data/repository.py for data access abstraction
- **Adapter pattern**: UnifiedSettings bridges new config system and legacy Settings
- **Singleton pattern**: Global instances for cache managers, profilers, connection pools
- **Observer pattern**: DataUpdateListener for cache invalidation on data updates
- **Template method pattern**: BaseTradeExecutor provides shared execution logic, BaseStrategy for strategy framework

### Performance Architecture
- **Connection pooling**: Reduces connection overhead, especially important for prediction cycles processing multiple symbols
- **Query caching**: Reduces database load for frequently-accessed data (dashboard queries, signal lookups)
- **Calculation caching**: Specialized cache for expensive calculations with intelligent invalidation
- **Profiling**: Built-in profiling for calculations and queries to identify bottlenecks
- **Optimizations**: Binary search for timestamp lookups, optimized clustering algorithms, early termination
- **Parallel processing**: Support for parallel batch processing in core/parallel_processor.py
- **Memory management**: Memory monitoring and GC optimization in core/io_optimizer.py
- **Target achieved**: <200ms per symbol/timeframe bundle through comprehensive optimization

### Portfolio Backtesting Architecture
- **Shared capital pool**: PortfolioPositionManager manages multiple positions sharing capital
- **Signal ranking**: SignalRanker prioritizes signals based on composite score (signal strength, risk/reward, confluence, trend alignment, volatility)
- **Position diversity**: Penalizes over-concentration in same sector
- **Confidence scaling**: Optional confidence-based position sizing
- **On-the-fly indicators**: PortfolioIndicatorCalculator calculates indicators during backtest without pre-computation
- **HTF caching**: Pre-loads higher timeframe data for all symbols to avoid repeated queries
- **Signal evaluation**: SignalEvaluator tracks predicted signals vs actual trade outcomes

---

End of llms.txt
